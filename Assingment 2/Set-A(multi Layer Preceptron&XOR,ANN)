Assignment 2
Set A)
Q1.Write a python program to implement XOR gate using a multi layer perception learning rule.
import numpy as np

# Define the XOR gate dataset (inputs and expected outputs)
X = np.array([[0, 0],
              [0, 1],
              [1, 0],
              [1, 1]])

y = np.array([[0], [1], [1], [0]])  # Expected outputs for XOR gate

# Initialize weights and biases randomly for the MLP
input_layer_neurons = 2  # Input layer has 2 neurons (2 features)
hidden_layer_neurons = 2  # Hidden layer has 2 neurons
output_neurons = 1  # Output layer has 1 neuron

# Initialize weights and biases randomly
hidden_weights = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))
hidden_bias = np.random.uniform(size=(1, hidden_layer_neurons))
output_weights = np.random.uniform(size=(hidden_layer_neurons, output_neurons))
output_bias = np.random.uniform(size=(1, output_neurons))

# Define the activation function (sigmoid function)
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

# Define the learning parameters
learning_rate = 0.1
max_epochs = 10000  # Maximum number of iterations

def mlp_train(X, y, hidden_weights, hidden_bias, output_weights, output_bias, learning_rate, max_epochs):
    for epoch in range(max_epochs):
        # Forward propagation
        hidden_layer_input = np.dot(X, hidden_weights) + hidden_bias
        hidden_layer_output = sigmoid(hidden_layer_input)

        output_layer_input = np.dot(hidden_layer_output, output_weights) + output_bias
        predicted_output = sigmoid(output_layer_input)

        # Backpropagation
        error = y - predicted_output
        d_predicted_output = error * sigmoid_derivative(predicted_output)

        error_hidden_layer = d_predicted_output.dot(output_weights.T)
        d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)

        # Update weights and biases
        output_weights += hidden_layer_output.T.dot(d_predicted_output) * learning_rate
        output_bias += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate
        hidden_weights += X.T.dot(d_hidden_layer) * learning_rate
        hidden_bias += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate

        # Print progress every 1000 epochs
        if epoch % 1000 == 0:
            loss = np.mean(np.square(error))
            print(f"Epoch {epoch + 1}, Loss: {loss:.4f}")

    return hidden_weights, hidden_bias, output_weights, output_bias

# Train the MLP
hidden_weights, hidden_bias, output_weights, output_bias = mlp_train(
    X, y, hidden_weights, hidden_bias, output_weights, output_bias, learning_rate, max_epochs)

# Test the MLP
def mlp_predict(X, hidden_weights, hidden_bias, output_weights, output_bias):
    hidden_layer_input = np.dot(X, hidden_weights) + hidden_bias
    hidden_layer_output = sigmoid(hidden_layer_input)

    output_layer_input = np.dot(hidden_layer_output, output_weights) + output_bias
    return sigmoid(output_layer_input)

# Predict the outputs for all inputs of the XOR gate
predictions = mlp_predict(X, hidden_weights, hidden_bias, output_weights, output_bias)

# Print the results
print("\nFinal hidden weights:\n", hidden_weights)
print("Final hidden bias:\n", hidden_bias)
print("Final output weights:\n", output_weights)
print("Final output bias:\n", output_bias)
print("\nPredictions for XOR gate:")
for i, input_set in enumerate(X):
    print(f"Input: {input_set} => Prediction: {predictions[i][0]:.4f}")


Q2.Implement a Multi layer preceptron using a tensortflow library and calculate the accuracy of the model on the testing data.(Use MNIST dataset).
# Import libraries
import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt

# Verify versions of NumPy and TensorFlow
print(f"NumPy version: {np.__version__}")
print(f"TensorFlow version: {tf.__version__}")

# Load MNIST dataset
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Normalize the input data to be in the range [0, 1]
X_train = X_train.astype('float32') / 255
X_test = X_test.astype('float32') / 255

# One-hot encode the output labels
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Define the MLP model
model = Sequential()
model.add(Flatten(input_shape=(28, 28)))  # Flatten the input images
model.add(Dense(128, activation='relu'))  # Hidden layer with 128 neurons
model.add(Dense(64, activation='relu'))   # Hidden layer with 64 neurons
model.add(Dense(10, activation='softmax'))  # Output layer with 10 neurons for 10 classes

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test))

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test)

print(f"Test Accuracy: {test_accuracy * 100:.2f}%")



Q3. Write a python program to implement handwritten digit classification digit classification using ANN.(Use MNIST dataset)
# Import necessary libraries
import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt

# Load MNIST dataset
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Display a few sample images with labels
plt.figure(figsize=(10, 4))
for i in range(10):
    plt.subplot(2, 5, i + 1)
    plt.imshow(X_train[i], cmap='gray')
    plt.title(f"Label: {y_train[i]}")
    plt.axis('off')
plt.suptitle("Sample Images from MNIST Dataset")
plt.show()

# Normalize the input data to be in the range [0, 1]
X_train = X_train.astype('float32') / 255
X_test = X_test.astype('float32') / 255

# One-hot encode the output labels
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Define the ANN model
model = Sequential()
model.add(Flatten(input_shape=(28, 28)))  # Flatten the input images to a 1D vector
model.add(Dense(256, activation='relu'))  # Hidden layer with 256 neurons
model.add(Dense(128, activation='relu'))  # Hidden layer with 128 neurons
model.add(Dense(10, activation='softmax'))  # Output layer with 10 neurons for 10 classes

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test))

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

# Plot the training history
plt.figure(figsize=(12, 5))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()


Q4. Write a python program to implement Regression Analysis using ANN.(use california_housing_train.csv)
# Import necessary libraries
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Load California housing dataset
data = pd.read_csv('/home/california_housing_train.csv')

# Display the first few rows of the dataset to understand the structure
print(data.head())

# Split the data into input features (X) and target (y)
X = data.drop('median_house_value', axis=1).values
y = data['median_house_value'].values

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the data (important for neural networks)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Define the ANN model
model = Sequential()
model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))  # Hidden layer with 64 neurons
model.add(Dense(32, activation='relu'))  # Hidden layer with 32 neurons
model.add(Dense(1))  # Output layer for regression (single value output)

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])

# Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model on the test set
test_loss, test_mse = model.evaluate(X_test, y_test)
print(f"Test Mean Squared Error: {test_mse:.2f}")

# Plot the training history
plt.figure(figsize=(12, 5))

# Plot Mean Squared Error
plt.plot(history.history['mean_squared_error'], label='Training MSE')
plt.plot(history.history['val_mean_squared_error'], label='Validation MSE')
plt.title('Mean Squared Error Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Mean Squared Error')
plt.legend()
plt.show()

# Predict on test data
y_pred = model.predict(X_test)

# Plot actual vs predicted values
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.xlabel('Actual Median House Value')
plt.ylabel('Predicted Median House Value')
plt.title('Actual vs Predicted House Values')
plt.show()


Q5.Write a python program to build a feed forward Neural network using Sigmoid activation function.(Use MNIST dataset)
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.utils import to_categorical
import numpy as np

# Load and preprocess the MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# One-hot encode the labels
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Define parameters
batch_size = 100
n_items = 3000
num_samples = len(x_train)  # Total number of training samples

# Calculate the number of epochs
num_epochs = n_items / (num_samples / batch_size)
num_epochs = int(num_epochs)

# Build the Feed-Forward Neural Network model with Sigmoid activation
model = Sequential([
    Flatten(input_shape=(28, 28)),         # Flatten the 28x28 images
    Dense(128, activation='sigmoid'),      # First hidden layer with sigmoid activation
    Dense(64, activation='sigmoid'),       # Second hidden layer with sigmoid activation
    Dense(10, activation='sigmoid')        # Output layer with sigmoid activation
])

# Train the model
model.fit(x_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_split=0.2)

# Evaluate the model
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f"Test accuracy: {test_accuracy * 100:.2f}%")
