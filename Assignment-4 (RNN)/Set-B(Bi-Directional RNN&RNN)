Set B)
1)Create a Bi-directional TNN model to perform sentiment analysis and predict the sentiment for a given user sentance and also compute the model accuracy and model loss with respect to number of epochs.
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing import sequence

# Load the IMDB dataset
max_features = 10000  # Number of unique words to consider
maxlen = 200  # Maximum length of each review
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)

# Pad sequences to ensure uniform input size
X_train = sequence.pad_sequences(X_train, maxlen=maxlen)
X_test = sequence.pad_sequences(X_test, maxlen=maxlen)

from tensorflow.keras import layers, models

# Define the Bi-directional RNN model
model = models.Sequential()
model.add(layers.Embedding(max_features, 128, input_length=maxlen))
model.add(layers.Bidirectional(layers.LSTM(64, return_sequences=False)))
model.add(layers.Dense(1, activation='sigmoid'))  # For binary classification

# Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, batch_size=64, epochs=5, validation_split=0.2)

# Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f'Test Loss: {test_loss}')
print(f'Test Accuracy: {test_accuracy}')


import matplotlib.pyplot as plt

# Plot training & validation accuracy values
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()


import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing import sequence
import matplotlib.pyplot as plt

# Load the IMDB dataset
max_features = 10000  # Number of unique words to consider
maxlen = 200  # Maximum length of each review
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)

# Pad sequences to ensure uniform input size
X_train = sequence.pad_sequences(X_train, maxlen=maxlen)
X_test = sequence.pad_sequences(X_test, maxlen=maxlen)

# Function to create and compile the Bidirectional LSTM model
def create_bi_lstm_model():
    model = tf.keras.models.Sequential()
    model.add(tf.keras.layers.Embedding(max_features, 128, input_length=maxlen))
    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))  # Bidirectional LSTM
    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # For binary classification
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# Function to create and compile the GRU model
def create_gru_model():
    model = tf.keras.models.Sequential()
    model.add(tf.keras.layers.Embedding(max_features, 128, input_length=maxlen))
    model.add(tf.keras.layers.GRU(64))  # GRU
    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # For binary classification
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# Train and evaluate the Bidirectional LSTM model
bi_lstm_model = create_bi_lstm_model()
bi_lstm_history = bi_lstm_model.fit(X_train, y_train, batch_size=64, epochs=2, validation_split=0.2)
bi_lstm_test_loss, bi_lstm_test_accuracy = bi_lstm_model.evaluate(X_test, y_test)

print(f'Bidirectional LSTM - Test Loss: {bi_lstm_test_loss}, Test Accuracy: {bi_lstm_test_accuracy}')

# Train and evaluate the GRU model
gru_model = create_gru_model()
gru_history = gru_model.fit(X_train, y_train, batch_size=64, epochs=2, validation_split=0.2)
gru_test_loss, gru_test_accuracy = gru_model.evaluate(X_test, y_test)

print(f'GRU - Test Loss: {gru_test_loss}, Test Accuracy: {gru_test_accuracy}')

# Plotting the results for Bidirectional LSTM
plt.figure(figsize=(12, 5))

# Plot Bidirectional LSTM training & validation accuracy
plt.subplot(1, 2, 1)
plt.plot(bi_lstm_history.history['accuracy'], label='Train')
plt.plot(bi_lstm_history.history['val_accuracy'], label='Validation')
plt.title('Bidirectional LSTM Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='upper left')

# Plot Bidirectional LSTM training & validation loss
plt.subplot(1, 2, 2)
plt.plot(bi_lstm_history.history['loss'], label='Train')
plt.plot(bi_lstm_history.history['val_loss'], label='Validation')
plt.title('Bidirectional LSTM Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='upper left')

plt.tight_layout()
plt.show()

# Plotting the results for GRU
plt.figure(figsize=(12, 5))

# Plot GRU training & validation accuracy
plt.subplot(1, 2, 1)
plt.plot(gru_history.history['accuracy'], label='Train')
plt.plot(gru_history.history['val_accuracy'], label='Validation')
plt.title('GRU Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='upper left')

# Plot GRU training & validation loss
plt.subplot(1, 2, 2)
plt.plot(gru_history.history['loss'], label='Train')
plt.plot(gru_history.history['val_loss'], label='Validation')
plt.title('GRU Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='upper left')

plt.tight_layout()
plt.show()




2) Write a python program to recognize handwritten digits using RNN
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense, Flatten

#Load the MNIST Dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

#Preprocess the Data
# Normalize pixel values to the range [0, 1]
x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2]) / 255.0
x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2]) / 255.0

# Convert class labels to one-hot encoding
num_classes = 10
y_train = tf.keras.utils.to_categorical(y_train, num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes)

#Create the RNN Model
model = Sequential()
model.add(SimpleRNN(units=64, input_shape=(x_train.shape[1], x_train.shape[2])))
model.add(Dense(num_classes, activation='softmax'))

#Compile the Model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

#Train the Model
epochs = 10
batch_size = 128
model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test))

#Evaluate the Model
loss, accuracy = model.evaluate(x_test, y_test)
print('Test accuracy:', accuracy)











#using Plot same above program

import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense, Flatten
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt

# Load and preprocess data
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train / 255.0
x_test = x_test / 255.0
x_train = np.expand_dims(x_train, axis=-1)  # Add channel dimension
x_test = np.expand_dims(x_test, axis=-1)  # Add channel dimension
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Build Simple RNN model
model = Sequential([
    SimpleRNN(64, input_shape=(28, 28), return_sequences=False),
    Dense(10, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)

# Evaluate the model
test_loss, test_acc = model.evaluate(x_test, y_test)
print(f"Test accuracy: {test_acc:.4f}")

# Plot training & validation accuracy values
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

